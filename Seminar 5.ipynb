{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar 5: Basic Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this seminar is simple, yet an actual implementation may take some time :). We are going to write an Artificial Neural Network (almost) from scratch. The software design of the following scaffolding was heavily inspired by [Caffe](http://caffe.berkeleyvision.org/) which is a good thing as this package will be used extensively later in the course. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from time import time, sleep\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptrons for digit classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we are using [MNIST](http://yann.lecun.com/exdb/mnist/) as our dataset. First 60,000 samples are intended for training and validation (for simplicity, we'll use the whole chunk for training), and the rest 10,000 is a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.datasets import fetch_mldata\n",
    "\n",
    "# Fetch MNIST dataset and create a local copy.\n",
    "if os.path.exists('mnist.npz'):\n",
    "    with np.load('mnist.npz', 'r') as data:\n",
    "        X = data['X']\n",
    "        y = data['y']\n",
    "else:\n",
    "    mnist = fetch_mldata(\"MNIST original\")\n",
    "    X, y = mnist.data / 255.0, mnist.target\n",
    "    np.savez('mnist.npz', X=X, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the modern Deep Learning packages defines a set of **layers** which are used as building blocks for the complete Neural Network. In a nut shell, layer is some kind of transformation applied to the input data. It should be differentiable for the back-propagation to work. As it is usually done, the data is stored in so-called **blobs**. In our case blobs are just pairs of regular 2D matrices of size $ (N \\times C) $, where $ N $ is the number of samples in a batch, $ C $ is the number of channels in a sample. Here is a small example of a blob:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 100\n",
    "C = 512\n",
    "\n",
    "blob = {\n",
    "    'data': np.zeros((N, C)),\n",
    "    'diff': np.zeros((N, C)),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above defines a blob for a batch of 100 samples with 512 channels per sample. The actual data is held in `blob['data']`, while `blob['diff']` is used as a storage for data gradients, i.e. $ \\frac{\\partial \\, \\mathtt{Objective}}{\\partial \\, \\mathtt{Data}} $.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code defines a common interface for every layer that we are going to use in our network. The **`bottom`** corresponds to the list of input blobs of the layer (some layers may receive multiple input blobs) and the **``top``** is a list (most commonly containing only one element) of output blobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "class BaseLayer:\n",
    "    __metaclass__ = ABCMeta\n",
    "    \n",
    "    @abstractmethod\n",
    "    def reshape(self, bottom, top):\n",
    "        \"\"\"\n",
    "        Reshapes (resizes) top blobs accoring to the size of bottom blobs and internal\n",
    "        parameters of the layer. The idea here is to allocate all the blobs only once\n",
    "        avoiding repeated creation of arrays during the optimization process.\n",
    "\n",
    "        Keyword arguments:\n",
    "        bottom -- list of input blobs\n",
    "        top    -- list of output blobs\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def fprop(self, bottom, top):\n",
    "        \"\"\"\n",
    "        Applies layer transformation to the list of input blobs and writes the results\n",
    "        to the list of the output blobs.\n",
    "\n",
    "        Keyword arguments:\n",
    "        bottom -- list of input blobs\n",
    "        top    -- list of output blobs\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def bprop(self, top, bottom):\n",
    "        \"\"\"\n",
    "        Computes (d Objective) / (d Data) for all input blobs and stores the result for\n",
    "        the i-th blob to bottom[i]['diff'].\n",
    "\n",
    "        Keyword arguments:\n",
    "        bottom -- list of input blobs\n",
    "        top    -- list of output blobs\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the interface, it's time implement a bunch of actual layers. It's up to you to define more modules. Here's the complete example for the **Rectified Linear Unit** non-linearity (aka **ReLU**): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ReLULayer(BaseLayer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def reshape(self, bottom, top):\n",
    "        top[0]['data'] = bottom[0]['data']\n",
    "        top[0]['diff'] = bottom[0]['diff']\n",
    "        self.mask = np.zeros(bottom[0]['data'].shape)\n",
    "    \n",
    "    def fprop(self, bottom, top):\n",
    "        np.maximum(bottom[0]['data'], 0.0, out=top[0]['data'])\n",
    "\n",
    "    def bprop(self, top, bottom):\n",
    "        self.mask[:] = bottom[0]['data'] > 0.0\n",
    "        np.multiply(top[0]['diff'], self.mask, out=bottom[0]['diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The following 3 functors are needed for the weight initialization.\n",
    "\n",
    "class ConstantFiller:\n",
    "    '''Returns a matrix filled with the specified value.'''\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        \n",
    "    def __call__(self, shape):\n",
    "        return self.value * np.ones(shape)\n",
    "\n",
    "class GaussianFiller:\n",
    "    '''Returns a matrix filled with a gaussian noise ~ N(mu, sigma).'''\n",
    "    def __init__(self, mu, sigma):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def __call__(self, shape):\n",
    "        return np.random.normal(self.mu, self.sigma, size=shape)\n",
    "    \n",
    "class XavierFiller:\n",
    "    '''\n",
    "    Returns a matrix filled uniform noise tailored to speed-up ANN learning.\n",
    "    \n",
    "    References:\n",
    "    * Glorot et al., 2010: \n",
    "    http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def __call__(self, shape):\n",
    "        fan_in = shape[0]\n",
    "        fan_out = shape[1]\n",
    "        delta = np.sqrt(6.0 / (fan_in + fan_out))\n",
    "        return np.random.uniform(-delta, delta, size=shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's your turn to implement the **Fully-connected** layer (``InnerProductLayer`` in the code) and the **Softmax + Multinomial Logistic Loss** combo (``SoftmaxLossLayer`` in the code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class InnerProductLayer(BaseLayer):\n",
    "    \"\"\"\n",
    "    Fully-connected layer. Computes W * input + b.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, filters, **keywords):\n",
    "        self.filters = filters\n",
    "        self.channels = channels\n",
    "        \n",
    "        self.params = {\n",
    "            'W': {\n",
    "                'data': np.zeros((filters, channels)),\n",
    "                'diff': np.zeros((filters, channels))\n",
    "            },\n",
    "            'b': {\n",
    "                'data': np.zeros((filters, 1)),\n",
    "                'diff': np.zeros((filters, 1))\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for k in self.params.keys():\n",
    "            self.params[k]['data'][:] = keywords[k](self.params[k]['data'].shape)\n",
    "            \n",
    "    def reshape(self, bottom, top):\n",
    "        top[0]['data'] = np.zeros((self.filters, bottom[0]['data'].shape[0]))\n",
    "        top[0]['diff'] = np.zeros((self.filters, bottom[0]['data'].shape[0]))\n",
    "    \n",
    "    def fprop(self, bottom, top):\n",
    "        np.dot(self.params['W']['data'], bottom[0]['data'].transpose(), out=top[0]['data'])\n",
    "        top[0]['data'] = top[0]['data'] + self.params['b']['data']\n",
    "\n",
    "    def bprop(self, top, bottom):\n",
    "        np.dot(top[0]['diff'], bottom[0]['data'], out=self.params['W']['diff'])\n",
    "        self.params['b']['diff'] = top[0]['diff']\n",
    "        np.dot(top[0]['diff'].transpose(), self.params['W']['data'], out=bottom[0]['diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SoftmaxLossLayer(BaseLayer):\n",
    "    \"\"\"\n",
    "    Computes the multinomial logistic loss of the softmax of its inputs. Itâ€™s \n",
    "    conceptually identical to a softmax layer followed by a multinomial logistic \n",
    "    loss layer, but provides a more numerically stable gradient.\n",
    "    \n",
    "    References:\n",
    "    * Softmax:\n",
    "    http://en.wikipedia.org/wiki/Softmax_function\n",
    "    * Multinomial logistic loss:\n",
    "    http://en.wikipedia.org/wiki/Multinomial_logistic_regression\n",
    "    http://caffe.berkeleyvision.org/doxygen/classcaffe_1_1SoftmaxWithLossLayer.html\n",
    "    http://caffe.berkeleyvision.org/doxygen/classcaffe_1_1MultinomialLogisticLossLayer.html#details\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def reshape(self, bottom, top):\n",
    "        top[0]['data'] = np.zeros((bottom[0]['data'].shape[0],1))\n",
    "        top[0]['diff'] = np.zeros((bottom[0]['data'].shape[0],1))\n",
    "        self.probs = np.zeros(bottom[0]['data'].shape)\n",
    "    \n",
    "    def fprop(self, bottom, top):\n",
    "        self.probs = np.exp(bottom[0]['data']) /  np.vstack(np.sum(np.exp(bottom[0]['data']), axis=1))\n",
    "        for i in xrange(bottom[0]['data'].shape[0]):\n",
    "            top[0]['data'][0] -= np.log(self.probs[i, bottom[1]['data'][i]]) \n",
    "        top[0]['data'][0] /= bottom[0]['data'].shape[0]\n",
    "\n",
    "    def bprop(self, top, bottom):\n",
    "        bottom[0]['diff'] = self.probs / bottom[0]['data'].shape[0]\n",
    "        for i in xrange(bottom[0]['data'].shape[0]):\n",
    "            bottom[0]['diff'][i, bottom[1]['data'][i]] -= 1. /  bottom[0]['data'].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a complete model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to assemble freshly coded layers into a complete model by means of additional class ``MLP`` (i.e. **Multi-layer perceptron**). Before we start, let's briefly look at how one would use it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's proceed to the implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\"\n",
    "    Multi-layer perceptron class.\n",
    "    \"\"\"\n",
    "    def __init__(self, desc):\n",
    "        self.layers = desc\n",
    "        self.reshaped = False\n",
    "        \n",
    "        # Here we aggregate all tunable parameters into a single list.\n",
    "        self.params = []\n",
    "        for l in self.layers:\n",
    "            try:\n",
    "                for v in l.params.values():\n",
    "                    self.params.append(v)\n",
    "            except Exception, e:\n",
    "                pass\n",
    "    \n",
    "    def reshape(self, input_shape):\n",
    "        \"\"\"\n",
    "        Invokes reshape methods of all layers.\n",
    "        \"\"\"\n",
    "        batch_size = input_shape[0]\n",
    "        self.data = {\n",
    "            'data': np.zeros(input_shape), \n",
    "            'diff': np.zeros(input_shape)\n",
    "        }\n",
    "        self.labels = {\n",
    "            'data': np.zeros((batch_size, 1), dtype=np.int32)\n",
    "        }\n",
    "        \n",
    "        self.blobs = [] # Holds all internal blobs created by reshape methods.\n",
    "        \n",
    "        # Your code goes here. ################################################ \n",
    "        self.blobs.append(self.data)\n",
    "        for layer in desc:\n",
    "            new_bottom = [{'data': None, 'diff': None}]\n",
    "            layer.reshape(self.blobs[-1:], new_bottom)\n",
    "            self.blobs.append(new_bottom[0])\n",
    "        # Your code goes here. ################################################ \n",
    "        \n",
    "        self.reshaped = True\n",
    "    \n",
    "    def set_input(self, data, labels):\n",
    "        if not self.reshaped:\n",
    "            self.reshape(data.shape)\n",
    "            \n",
    "        # Populate self.data and self.labels.\n",
    "        # Your code goes here. ################################################\n",
    "        self.data = {'data':data, 'diff':np.zeros(data.shape)}\n",
    "        self.labels = {'data':labels}\n",
    "        # Your code goes here. ################################################\n",
    "        \n",
    "    def fprop(self):\n",
    "        \"\"\"\n",
    "        Conducts forward-propagation through the network.\n",
    "        \n",
    "        (i.e. fills self.blobs[:]['data'])\n",
    "        \"\"\"\n",
    "        # Your code goes here. ################################################ \n",
    "        for i in xrange(len(self.layers) - 1):\n",
    "            self.layers[i].fprop(self.blobs[i:i+1], self.blobs[i+1:i+2])\n",
    "        self.layers[-1].fprop([self.blobs[len(self.layers) - 1], self.labels], \\\n",
    "                               self.blobs[len(self.layers):])\n",
    "        # Your code goes here. ################################################ \n",
    "        # NOTE: Keep in mind that the last layer should receive ground-truth \n",
    "        #       labels as well as blob from the lower layer.\n",
    "        \n",
    "    def bprop(self):\n",
    "        \"\"\"\n",
    "        Conducts backward-propagation through the network.\n",
    "        \n",
    "        (i.e. fills self.blobs[:]['diff'] and updates 'diff's of the internal \n",
    "        weight blobs)\n",
    "        \"\"\"\n",
    "        # Your code goes here. ################################################ \n",
    "        self.layers[-1].bprop(self.blobs[len(self.layers):], [self.blobs[len(self.layers)-1], self.labels])\n",
    "        for i in xrange(1, len(self.layers)):\n",
    "            self.layers[len(self.layers) - 1 - i].bprop(\\\n",
    "                                    self.blobs[len(self.blobs) - 1 - i:len(self.blobs) - i], \\\n",
    "                                    self.blobs[len(self.blobs) - 2 - i:len(self.blobs) - 1 - i])\n",
    "        for i in xrange(len(self.params)):\n",
    "            self.params[i]['data'] -= self.params[i]['diff']\n",
    "        # Your code goes here. ################################################ \n",
    "        \n",
    "    def get_loss(self):\n",
    "        \"\"\" Return the value of the objective function \"\"\"\n",
    "        return self.blobs[-1]['data'].mean()\n",
    "    \n",
    "    def test(self, data, labels):\n",
    "        \"\"\"\n",
    "        Helper function for evaluating the performance of the network on a test\n",
    "        set (which can be larger than the batch size).\n",
    "        \n",
    "        Returns accuracy.\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = self.data['data'].shape[0]\n",
    "        preds = []\n",
    "        for start in xrange(0, data.shape[0], batch_size):\n",
    "            self.set_input(data[start : start + batch_size, :], labels[start : start + batch_size])\n",
    "            self.fprop()\n",
    "            preds += [self.layers[-1].probs.argmax(axis=1)]\n",
    "        preds = np.hstack(preds)\n",
    "        return np.mean(preds == labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's sometimes useful to look at the network weights as that may give some insights of what the ANN has learned during training. The following code has been shamelessly copy-and-pasted from the original ``Caffe`` tutorial. We will be using that to visualize the state of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# take an array of shape (n, height, width) or (n, height, width, channels)\n",
    "# and visualize each (height, width) thing in a grid of size approx. sqrt(n) by sqrt(n)\n",
    "def vis_square(data, padsize=1, padval=0):\n",
    "    data = np.copy(data)\n",
    "    data -= data.min()\n",
    "    data /= data.max()\n",
    "    \n",
    "    # force the number of filters to be square\n",
    "    n = int(np.ceil(np.sqrt(data.shape[0])))\n",
    "    padding = ((0, n ** 2 - data.shape[0]), (0, padsize), (0, padsize)) + ((0, 0),) * (data.ndim - 3)\n",
    "    data = np.pad(data, padding, mode='constant', constant_values=(padval, padval))\n",
    "    \n",
    "    # tile the filters into an image\n",
    "    data = data.reshape((n, n) + data.shape[1:]).transpose((0, 2, 1, 3) + tuple(range(4, data.ndim + 1)))\n",
    "    data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:])\n",
    "    \n",
    "    plt.imshow(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have implemented everything we need in order to finally train our ANN. We will be using **Stochastic Gradient Descent** (SGD) with momentum as the optimization algorithm. That should be familiar to you from the previous lecture. Compare single-layer linear perceptron and true multi-layer perceptron (say, with **500** hidden units and ReLU non-linearity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's define the parameters of the optimization procedure.\n",
    "# Your code goes here. ######################################################## \n",
    "# n_epochs     = # This many times we will go through the whole training set\n",
    "# batch_size   = # This many samples are packed into a single batch\n",
    "n_epochs = 5\n",
    "batch_size = 5000\n",
    "\n",
    "# lr           = # Learning rate.\n",
    "# momentum     = # Momentum.\n",
    "# weight_decay = # L2-regularization coefficient.\n",
    "lr = 0.1\n",
    "momentum = 0.85\n",
    "weight_decay = 0.05\n",
    "\n",
    "# test_iter    = # Perform testing of the model every this many iterations.\n",
    "test_iter = 10\n",
    "\n",
    "n_samples = 60000\n",
    "n_batches_per_epoch = n_samples / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a sequence of layers.\n",
    "# Your code goes here. ######################################################## \n",
    "# desc = []\n",
    "desc = [\\\n",
    "    InnerProductLayer(X.shape[1], 10, W=XavierFiller(), b=ConstantFiller(0.0)),\\\n",
    "    SoftmaxLossLayer()\\\n",
    "]\n",
    "\n",
    "model = MLP(desc)\n",
    " \n",
    "training_loss = {\n",
    "    'ts': [],\n",
    "    'values': []\n",
    "}\n",
    "test_accuracy = {\n",
    "    'ts': [],\n",
    "    'values': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "thetime = time()\n",
    "\n",
    "indices = np.arange(n_samples)\n",
    "for t in xrange(n_epochs * n_batches_per_epoch):\n",
    "    i = t % n_batches_per_epoch\n",
    "    \n",
    "    # Shuffle dataset after each epoch.\n",
    "    if i == 0:\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "    # Perform testing (using model.test)\n",
    "    if t % test_iter == 0 and t != 0:\n",
    "        # Your code goes here. ################################################ \n",
    "        test_accuracy['ts'].append(t)\n",
    "        test_accuracy['values'].append(model.test(X[-10000:,:],y[-10000:]))\n",
    "        # Your code goes here. ################################################ \n",
    "    \n",
    "    start = i * batch_size\n",
    "    end = (i + 1) * batch_size\n",
    "    batch_indices = indices[start : end]\n",
    "    \n",
    "    batch = (X[batch_indices, :], np.int32(y[batch_indices]))\n",
    "    \n",
    "    # Your code goes here. ####################################################\n",
    "    model.set_input(batch[0], batch[1])\n",
    "    model.fprop()\n",
    "    model.bprop()\n",
    "    training_loss['ts'].append(t)\n",
    "    training_loss['values'].append(model.get_loss())\n",
    "    # Your code goes here. ################################################ \n",
    "    \n",
    "    # Visualize training process.\n",
    "    display.clear_output(wait=True)\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    \n",
    "    gs = gridspec.GridSpec(1, 3, width_ratios=[1, 2, 2]) \n",
    "    \n",
    "    ax = plt.subplot(gs[0])\n",
    "    ax.set_title(\"Weights\")\n",
    "    vis_square(model.layers[0].params['W']['data'].reshape((-1, 28, 28))[: 25, :, :])\n",
    "\n",
    "    ax = plt.subplot(gs[1])\n",
    "    ax.set_title(\"Training loss\")\n",
    "    plt.plot(training_loss['ts'], training_loss['values'], 'b')\n",
    "    \n",
    "    ax = plt.subplot(gs[2])\n",
    "    ax.set_title(\"Test accuracy\")\n",
    "    plt.plot(test_accuracy['ts'], test_accuracy['values'], 'r')\n",
    "    \n",
    "    plt.show()\n",
    "print time() - thetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-17-923a72fd3a1d>, line 52)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-17-923a72fd3a1d>\"\u001b[1;36m, line \u001b[1;32m52\u001b[0m\n\u001b[1;33m    plt.plot(test_accuracy['ts']), test_accuracy['values'], 'r')\u001b[0m\n\u001b[1;37m                                                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Define a sequence of layers.\n",
    "# Your code goes here. ######################################################## \n",
    "# desc = []\n",
    "desc = [\\\n",
    "    InnerProductLayer(X.shape[1], 500, W=XavierFiller(), b=ConstantFiller(0.0)),\\\n",
    "    ReluLayer(),\\\n",
    "    InnerProductLayer(500, 10, W=XavierFiller(), b=ConstantFiller(0.0)),\\    \n",
    "    SoftmaxLossLayer()\\\n",
    "]\n",
    "\n",
    "model = MLP(desc)\n",
    " \n",
    "training_loss = {\n",
    "    'ts': [],\n",
    "    'values': []\n",
    "}\n",
    "test_accuracy = {\n",
    "    'ts': [],\n",
    "    'values': []\n",
    "}\n",
    "\n",
    "indices = np.arange(n_samples)\n",
    "for t in xrange(n_epochs * n_batches_per_epoch):\n",
    "    i = t % n_batches_per_epoch\n",
    "    \n",
    "    # Shuffle dataset after each epoch.\n",
    "    if i == 0:\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "    # Perform testing (using model.test)\n",
    "    if t % test_iter == 0 and t != 0:\n",
    "        model.test()\n",
    "        # Your code goes here. ################################################ \n",
    "    \n",
    "    start = i * batch_size\n",
    "    end = (i + 1) * batch_size\n",
    "    batch_indices = indices[start : end]\n",
    "    \n",
    "    batch = (X[batch_indices, :], np.int32(y[batch_indices]))\n",
    "    \n",
    "    \n",
    "    # Your code goes here. ####################################################\n",
    "    \n",
    "    # Visualize training process.\n",
    "    display.clear_output(wait=True)\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    \n",
    "    gs = gridspec.GridSpec(1, 3, width_ratios=[1, 2, 2]) \n",
    "    \n",
    "    ax = plt.subplot(gs[0])\n",
    "    ax.set_title(\"Weights\")\n",
    "    vis_square(model.layers[0].params['W']['data'].reshape((-1, 28, 28))[: 25, :, :])\n",
    "\n",
    "    ax = plt.subplot(gs[1])\n",
    "    ax.set_title(\"Training loss\")\n",
    "    plt.plot(training_loss['ts'], training_loss['values'], 'b')\n",
    "    \n",
    "    ax = plt.subplot(gs[2])\n",
    "    ax.set_title(\"Test accuracy\")\n",
    "    plt.plot(test_accuracy['ts'], test_accuracy['values'], 'r')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are experiencing problems with convergence, try applying a schedule, e.g. something like `inv`-policy from `Caffe`:\n",
    "$$\n",
    "    \\mathtt{base\\_lr} \\cdot (1 + \\gamma \\cdot \\mathtt{t})^{-p} \\, .\n",
    "$$\n",
    "See `Caffe` [examples](https://github.com/BVLC/caffe/blob/master/examples/mnist/lenet_solver.prototxt) for typical values of $ \\gamma $ and $ p $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What has it learned?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a learned classification model and a class of interest one may numerically generate an image, which is representative of the class in terms of the ANN class scoring model.\n",
    "\n",
    "More formally, let $ S_c(I) $ be the score of the class $ c $, computed by the classification layer of the network for an image $ I $. We would like to find an $ \\mathcal{L}_2 $-regularized image, such that the score $ S_c $ is high:\n",
    "$$\n",
    "    \\arg\\max_{I} S_c(I) - \\lambda \\| I \\|_2^2 \\, ,\n",
    "$$\n",
    "where $ \\lambda $ is the regularisation parameter.\n",
    "\n",
    "For additional details see the paper by [Simonyan et al., 2010](http://www.robots.ox.ac.uk/~vedaldi/assets/pubs/simonyan14deep.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your task** is to get those reconstructed images for all 10 digits using previously trained multi-layer perceptron. This can be done by slightly modifying the optimization procedure implemented above. The following functions may come handy in that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dump_weights(filename, model):\n",
    "    \"\"\" Dumps all tunable parameters of the model into a file. \"\"\"\n",
    "    with open(filename, 'wb') as f:\n",
    "        for param in model.params:\n",
    "            for v in param.values():\n",
    "                np.save(f, v)\n",
    "                \n",
    "def load_weights(filename, model):\n",
    "    \"\"\" \n",
    "    Loads all tunable parameters from file into the specified model. \n",
    "    \n",
    "    NOTE: The model doesn't have to be of the same architecture as the model\n",
    "          that was used to dump the parameters. The only requirement is\n",
    "          the order of the tunable parameters.\n",
    "    \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        for param in model.params:\n",
    "            for v in param.values():\n",
    "                v[:] = np.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few tips:\n",
    "* You may want to define an additional loss function corresponding to value of the neuron.\n",
    "* As it pointed out in the original paper it better to optimize the output of the last linear layer rather than the output of Softmax.\n",
    "* Try starting with a zero image.\n",
    "\n",
    "Give few comments on obtained reconstructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
